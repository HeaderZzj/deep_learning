"""This module stores utility methods and configs for use by ICNet."""

import logging

import scipy.io as sio
import numpy as np
import tensorflow as tf


cityscape_label_colours = [[128, 64, 128], [244, 35, 231], [69, 69, 69],
                           # 0 = road, 1 = sidewalk, 2 = building
                           [102, 102, 156], [190, 153, 153], [153, 153, 153],
                           # 3 = wall, 4 = fence, 5 = pole
                           [250, 170, 29], [219, 219, 0], [106, 142, 35],
                           # 6 = traffic light, 7 = traffic sign, 8 = vegetation
                           [152, 250, 152], [69, 129, 180], [219, 19, 60],
                           # 9 = terrain, 10 = sky, 11 = person
                           [255, 0, 0], [0, 0, 142], [0, 0, 69],
                           # 12 = rider, 13 = car, 14 = truck
                           [0, 60, 100], [0, 79, 100], [0, 0, 230],
                           # 15 = bus, 16 = train, 17 = motocycle
                           [119, 10, 32]]
                           # 18 = bicycle

binary_label_colours = [[0, 0, 0], [255, 255, 255]]

ADE20K_COLOR_MAP = './color150.mat'

def set_logging_verbosity(verbose_level=3):
    """Set the level of logging verbosity."""

    if not isinstance(verbose_level, int):
        raise TypeError("verbose_level must be an int")

    if not (0 <= verbose_level <= 4):
        raise ValueError("verbose_level must be between 0 and 4")

    verbosity = [logging.CRITICAL,
                 logging.ERROR,
                 logging.WARNING,
                 logging.INFO,
                 logging.DEBUG]

    logging.basicConfig(
        format='%(asctime)s:\t %(message)s',
        datefmt='%m/%d/%Y %I:%M:%S %p',
        level=verbosity[verbose_level])

def zero_padding(input, paddings):
    """Zero padding layer (assumes NHWC format)."""
    pad_mat = np.array([[0,0], [paddings, paddings], [paddings, paddings], [0, 0]])
    return tf.pad(input, paddings=pad_mat)

def build_tfrecords_batch(*paths_to_tfrecords, input_shape, batch_size,
                          min_after_dequeue, max_iterations):
    """Build tensors for a batch of data given the path to tfrecord file"""
    filename_queue = tf.train.string_input_producer(
            [*paths_to_tfrecords], num_epochs=max_iterations)

    image_batch, label_batch = symbolic_read_and_decode_semseg_tfrecord(
            filename_queue, input_shape, batch_size=batch_size,
            num_threads=4, min_after_dequeue=min_after_dequeue)

    return image_batch, label_batch

def symbolic_read_and_decode_semseg_tfrecord(
    filename_queue, resize_dims, batch_size, num_threads, min_after_dequeue):
    """
    Given a TFRecord file generated by `kml.utils.serialize.semseg_files_to_tfrecords`,
    construct symbolic (image, mask) batches from it, adding all
    relevant ops to the graph.
    """

    reader = tf.TFRecordReader()

    _, serialized_example = reader.read(filename_queue)

    features = tf.parse_single_example(
      serialized_example,
      features={
        'original_height': tf.FixedLenFeature([], tf.int64),
        'original_width': tf.FixedLenFeature([], tf.int64),
        'resized_height': tf.FixedLenFeature([], tf.int64),
        'resized_width': tf.FixedLenFeature([], tf.int64),
        'image_raw': tf.FixedLenFeature([], tf.string),
        'mask_raw': tf.FixedLenFeature([], tf.string)})

    # Decode the image/mask back from bytes to uint8
    image = tf.decode_raw(features['image_raw'], tf.uint8)
    mask = tf.decode_raw(features['mask_raw'], tf.uint8)

    # Reshape to the image's original shape
    image = tf.reshape(image, (*resize_dims, 3))
    mask = tf.reshape(mask, (*resize_dims, 1))

    # Use capacity formula recommended by tensorflow
    capacity = min_after_dequeue + (num_threads + 1) * batch_size

    # Add queue/shuffling related ops to the graph
    image_batch, mask_batch= tf.train.shuffle_batch(
        [image, mask], batch_size=batch_size, capacity=capacity,
        num_threads=num_threads, min_after_dequeue=min_after_dequeue)

    return image_batch, mask_batch

def read_and_decode_semseg_tfrecord(path_to_semseg_tfrecords):
    """Generate samples from a semseg TFRecords file sequentially."""
    record_iterator = tf.python_io.tf_record_iterator(path=path_to_semseg_tfrecords)

    for string_record in record_iterator:

        example = tf.train.Example()
        example.ParseFromString(string_record)

        original_height = int(example.features.feature['original_height'].int64_list.value[0])
        original_width = int(example.features.feature['original_width'].int64_list.value[0])

        resized_height = int(example.features.feature['resized_height'].int64_list.value[0])
        resized_width = int(example.features.feature['resized_width'].int64_list.value[0])

        img_string = (example.features.feature['image_raw'].bytes_list.value[0])
        mask_string = (example.features.feature['mask_raw'].bytes_list.value[0])

        img_1d = np.fromstring(img_string, dtype=np.uint8)
        reconstructed_img = img_1d.reshape((resized_height, resized_width, -1))

        mask_1d = np.fromstring(mask_string, dtype=np.uint8)
        reconstructed_mask = mask_1d.reshape((resized_height, resized_width, -1))

        yield reconstructed_img, reconstructed_mask

def read_labelcolours(matfn):
    mat = sio.loadmat(matfn)
    color_table = mat['colors']
    shape = color_table.shape
    color_list = [tuple(color_table[i]) for i in range(shape[0])]

    return color_list

def decode_labels(mask, img_shape, num_classes):
    """Convert a channel mask given by a tensor into a colored image."""
    if num_classes == 2:
        label_colours = binary_label_colours
    elif num_classes == 19:
        label_colours = cityscape_label_colours
    # We need to add void label for ade20k since it's 0 (cityscapes void label
    # is 255; we can easily ignore it without affecting other labels/having to
    # remap)
    elif num_classes == 151:
        label_colours = [[0, 0, 0]] + read_labelcolours(ADE20K_COLOR_MAP)
    # If training on a custom dataset, you must provide a map from label to
    # color for visualization/output
    else:
        raise ValueError

    assert len(label_colours) == num_classes

    color_table = label_colours
    color_mat = tf.constant(color_table, dtype=tf.float32)
    onehot_output = tf.one_hot(mask, depth=num_classes)
    onehot_output = tf.reshape(onehot_output, (-1, num_classes))
    pred = tf.matmul(onehot_output, color_mat)
    pred = tf.reshape(pred, (1, img_shape[0], img_shape[1], 3))

    return pred

def prepare_label(input_batch, new_size, num_classes, one_hot=True):
    """Reshape label into correct size for corresponding logits given by new_size."""
    with tf.name_scope('label_encode'):
        # As labels are integer numbers, need to use NN interp.
        input_batch = tf.image.resize_nearest_neighbor(input_batch, new_size)
        input_batch = tf.squeeze(input_batch, squeeze_dims=[3])
        if one_hot:
            input_batch = tf.one_hot(input_batch, depth=num_classes)

    return input_batch

def get_mask(gt, num_classes, ignore_label):
    """Form mask with option to ignore a label."""
    less_equal_class = tf.less_equal(gt, num_classes-1)
    not_equal_ignore = tf.not_equal(gt, ignore_label)
    mask = tf.logical_and(less_equal_class, not_equal_ignore)
    indices = tf.squeeze(tf.where(mask), 1)

    return indices

def create_loss(output, label, num_classes, ignore_label):
    """
    Given output(logits) and labels, create a reduced softmax-xentropy loss.

    This is necessary as multiple losses with different logits dimensionality
    are required to form total loss, hence we have a helper do necessary
    resizing/shaping before creating the loss for a single branch of ICNet.
    """

    raw_pred = tf.reshape(output, [-1, num_classes])
    label = prepare_label(label, tf.stack(tf.shape(output)[1:3]),
                          num_classes=num_classes, one_hot=False)

    label = tf.reshape(label, [-1,])

    indices = get_mask(label, num_classes, ignore_label=ignore_label)
    gt = tf.cast(tf.gather(label, indices), tf.int32)
    pred = tf.gather(raw_pred, indices)

    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=gt)
    reduced_loss = tf.reduce_mean(loss)

    return reduced_loss
